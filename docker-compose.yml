services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: ppt-translate-backend
    ports:
      - "5001:5001"
    extra_hosts:
      # 讓容器可以連接到本機服務 (包括本機 Ollama)
      - "host.docker.internal:host-gateway"
    environment:
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - TRANSLATE_LLM_MODE=${TRANSLATE_LLM_MODE:-real}
      # 連接本機 Ollama (使用 host.docker.internal)
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2.5:7b}
      - OLLAMA_TIMEOUT=${OLLAMA_TIMEOUT:-180}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - GEMINI_MODEL=${GEMINI_MODEL:-gemini-1.5-flash}
      - GEMINI_TIMEOUT=${GEMINI_TIMEOUT:-180}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o-mini}
      - OPENAI_TIMEOUT=${OPENAI_TIMEOUT:-60}
      - SOURCE_LANGUAGE=${SOURCE_LANGUAGE:-auto}
      - LLM_CONTEXT_STRATEGY=${LLM_CONTEXT_STRATEGY:-none}
      - LLM_FALLBACK_ON_ERROR=${LLM_FALLBACK_ON_ERROR:-0}
      - LLM_SINGLE_REQUEST=${LLM_SINGLE_REQUEST:-1}
      - LLM_CHUNK_SIZE=${LLM_CHUNK_SIZE:-40}
      - LLM_MAX_RETRIES=${LLM_MAX_RETRIES:-2}
      - CORS_ORIGIN=${CORS_ORIGIN:-*}
    volumes:
      - ./data:/app/data
      - ./backend:/app/backend
      - ./docs:/app/docs
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:5001/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: ppt-translate-frontend
    ports:
      - "5193:80"
    depends_on:
      backend:
        condition: service_healthy
    volumes:
      - ./frontend:/app/frontend
    restart: unless-stopped

networks:
  default:
    name: ppt-translate-network
